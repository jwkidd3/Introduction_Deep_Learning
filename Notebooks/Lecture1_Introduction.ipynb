{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(As always a new environment specifically for this course is recommended!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Handling Data in PyTorch\n",
    "\n",
    "The torch ```tensor``` is the fundamental datatype used by PyTorch\n",
    "- works very similar to arrays\n",
    "- designed to work with GPUs\n",
    "- optimized for automatic differentiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# array\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# array\n",
    "X_array = np.array([[1,0],[0,1]])\n",
    "\n",
    "# tensor\n",
    "X_tensor = torch.tensor([[1,0],[0,1]])\n",
    "\n",
    "X_array, X_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# can easily convert back and forth\n",
    "X_tensor.numpy(), torch.from_numpy(X_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you want to use a GPU, you can use one for free in Google Colab or Kaggle for a limited amount of time each week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if a GPU is available we can send the tensor to the GPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(0)\n",
    "    X_tensor_cuda = X_tensor.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Dataset``` is an abstract class which holds the recipe for producing your data\n",
    "- can do complex operations to retrieve/transform your data in parallel\n",
    "- You must implement the following methods:\n",
    " - ```__init__```\n",
    " - ```__len__```: length of the dataset\n",
    " - ```__getitem__```: recipe for retrieving the *i*-th datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "\n",
    "# create some linear data on [0,10] according to a slope, intercept, and number of desired points\n",
    "def random_linear_data(m, b, n):\n",
    "    x = 10 * np.random.rand(n)\n",
    "    y = m * x + b + np.random.rand(n)\n",
    "    return x, y\n",
    "\n",
    "# create a dataset class\n",
    "class LinearDataset(Dataset):\n",
    "    # things I need to intialize\n",
    "    def __init__(self, m, b, n):\n",
    "        x, y = random_linear_data(m, b, n)\n",
    "        self.x, self.y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "        self.n = n\n",
    "        \n",
    "    # length of the dataset\n",
    "    def __len__(self):\n",
    "        return self.n\n",
    "    \n",
    "    # how to get a datapoint\n",
    "    # any transformations you want to do on-the-fly\n",
    "    def __getitem__(self, idx):\n",
    "        return self.x[idx], self.y[idx]\n",
    "    \n",
    "linear_ds = LinearDataset(1.5, 50, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get first datapoint\n",
    "next(iter(linear_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "Y = []\n",
    "# iterate through the dataset\n",
    "for x, y in linear_ds:\n",
    "    X.append(x.item())\n",
    "    Y.append(y.item())\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.scatter(X, Y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# turn iris data into a Dataset\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "\n",
    "iris = sns.load_dataset('iris')\n",
    "iris = iris[iris.species != 'virginica']\n",
    "iris.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IrisDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.df = df\n",
    "        self.species_val = {'setosa':0,\n",
    "                            'versicolor':1}\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # everything in getitem is done \"on-the-fly\"\n",
    "        \n",
    "        row = self.df.iloc[idx]\n",
    "        x = torch.tensor([row['sepal_length'],\n",
    "                          row['sepal_width']]).float()\n",
    "        \n",
    "        y = torch.tensor(self.species_val[row['species']]).float()\n",
    "        \n",
    "        return x, y\n",
    "    \n",
    "iris_ds = IrisDataset(iris)\n",
    "next(iter(iris_ds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general a good rule of thumb for what to do on-the-fly vs. preprocessing:\n",
    "- If it is random alteration (data augmentation): on-the-fly\n",
    "- If it is a time-consuming step that is also the same each time: preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ```Dataloader``` helps us iterate over a Dataset\n",
    "- can choose batch size\n",
    "- can shuffle\n",
    "- can be retrieved in parallel\n",
    "- automatically collates tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "iris_dl = DataLoader(iris_ds, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = next(iter(iris_dl))\n",
    "print(x.shape, y.shape)\n",
    "print(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a Model\n",
    "\n",
    "Let's define a simple Feed Forward neural network for the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class TwoLayerNN(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(TwoLayerNN, self).__init__()\n",
    "        \n",
    "        # initialize the layers with random weights\n",
    "        self.linear1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, output_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # define the actual function\n",
    "        x = self.linear1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        \n",
    "        # don't worry about the last activation function for now\n",
    "        return torch.squeeze(x)\n",
    "        \n",
    "model = TwoLayerNN(2, 5, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the attached gradient function below. PyTorch autograd is keeping track of the computational graph for computing partial derivatives with respect to the various parameters/weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x, y = next(iter(iris_dl))\n",
    "model(x), y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some very useful tools for looking at models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model, input_size = (2,), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# uh oh\n",
    "summary(model, input_size = (3,), device='cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchviz import make_dot\n",
    "make_dot(model(x), params=dict(list(model.named_parameters())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if the function is straightforward we can just use Sequential\n",
    "#model = nn.Sequential(nn.Linear(2, 5),\n",
    "#                      nn.ReLU(),\n",
    "#                      nn.Linear(5, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Model\n",
    "We need the following ingredients\n",
    "- A loss function for our model\n",
    "- An optimization algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# feeds outputs through a sigmoid before computing BCE Loss\n",
    "lossFun = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr = 0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we adjust the weights of the model according to one batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adjust the gradients according to one batch\n",
    "\n",
    "x, y = next(iter(iris_dl))\n",
    "\n",
    "# some layers will do different things during training/prediction (i.e. dropout)\n",
    "model.train()\n",
    "\n",
    "# compute the predictions then loss\n",
    "y_pred = model(x)\n",
    "loss = lossFun(y_pred, y)\n",
    "print(loss.item())\n",
    "\n",
    "# zero out the gradients in the optimizer (otherwise they will accumulate)\n",
    "optimizer.zero_grad()\n",
    "\n",
    "# compute the gradients w.r.t. loss function\n",
    "loss.backward()\n",
    "\n",
    "# adjust weights!\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An *epoch* is one pass through the training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# very crude training loop (you'll make a fancier one in your first lab)\n",
    "for epoch in range(100):\n",
    "    for x, y in iris_dl:\n",
    "        model.train()\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        loss = lossFun(y_pred, y)\n",
    "        print(loss.item())\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After Training\n",
    "Let's use our model to make some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y = next(iter(iris_dl))\n",
    "\n",
    "# some layers will do different things during training/prediction (i.e. dropout)\n",
    "model.eval()\n",
    "\n",
    "# don't compute gradients\n",
    "with torch.no_grad():\n",
    "    outputs = torch.sigmoid(model(x))\n",
    "\n",
    "y_pred = torch.zeros(10)\n",
    "y_pred[outputs > .5] = 1\n",
    "\n",
    "y_pred, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save your model parameters and optimizater checkpoint\n",
    "checkpoint = {'model_state_dict': model.state_dict(),\n",
    "              'optimizer_state_dict' :optimizer.state_dict()}\n",
    "torch.save(checkpoint, 'model_checkpoint.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now load them up!\n",
    "checkpoint = torch.load('model_checkpoint.pt')\n",
    "model.load_state_dict(checkpoint['model_state_dict'])\n",
    "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can save other things in the checkpoint such as the loss history, epoch number, etc. if you really want to save every aspect of your progress."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tip: Custom Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class some_loss(nn.Module):\n",
    "    def __init__(self, hyperparam):\n",
    "        super(some_loss, self).__init__()\n",
    "        self.hyperparam = hyperparam\n",
    "        \n",
    "    \n",
    "    def forward(self, y_pred, y):\n",
    "        diff = y_pred - y\n",
    "        \n",
    "        # average over each entry and batch size\n",
    "        torch.norm(diff) / torch.numel(doff)\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
